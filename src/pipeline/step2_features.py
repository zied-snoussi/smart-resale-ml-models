import pandas as pd
import joblib
import os
import logging
from utils.feature_utils import (
    extract_numerical_features, 
    process_text_vectors, 
    scale_features
)

# ==============================================================================
# CONFIGURATION DU LOGGING
# ==============================================================================
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def run_step2_features():
    """
    √âTAPE 2 : ING√âNIERIE DES CARACT√âRISTIQUES (FEATURE ENGINEERING)
    --------------------------------------------------------------
    Objectif : Transformer le texte en vecteurs s√©mantiques (SVD) et 
    standardiser les donn√©es num√©riques pour optimiser l'apprentissage.
    """
    logging.info("üöÄ D√âMARRAGE DE L'√âTAPE 2 : EXTRACTION DES CARACT√âRISTIQUES")

    # 1. CHARGEMENT DES DONN√âES PR√âPAR√âES (√âTAPE 1)
    input_path = 'data/processed/ebay_prep.pkl'
    if not os.path.exists(input_path):
        logging.error("‚ùå Fichier de pr√©paration introuvable ! Veuillez ex√©cuter l'√âtape 1 d'abord.")
        return
    
    df = pd.read_pickle(input_path)

    # 2. EXTRACTION DES CARACT√âRISTIQUES NUM√âRIQUES
    # G√©n√®re des variables comme 'is_bundle', 'title_length', ou 'msrp'.
    logging.info("Extraction des variables num√©riques et cat√©gorielles...")
    df_features = extract_numerical_features(df)

    # 3. VECTORISATION DU TEXTE (NLP PROFESSIONNEL)
    # Utilise TF-IDF + SVD pour transformer le 'Titre' en 20 colonnes num√©riques denses.
    # Cela permet de capturer la s√©mantique (ex: "neuf" vs "occasion") sans avoir des milliers de colonnes.
    logging.info("G√©n√©ration des vecteurs s√©mantiques (SVD) √† partir des titres...")
    X_text, vectorizer_artifacts = process_text_vectors(df['Title'])
    
    # 4. COMBINAISON ET NORMALISATION (SCALING)
    # Fusion des caract√©ristiques num√©riques et des vecteurs textuels.
    X_combined = pd.concat([df_features, X_text], axis=1)
    y = df['price_cleaned'] # Notre variable cible (Target)
    
    # Le StandardScaler est crucial pour que les variables √† grande √©chelle 
    # n'√©crasent pas les variables binaires lors de l'entra√Ænement.
    logging.info(f"Normalisation des {X_combined.shape[1]} caract√©ristiques...")
    X_scaled, scaler = scale_features(X_combined)

    # 5. SAUVEGARDE DES ARTEFACTS ET TRANSFORMATEURS
    # On s√©pare les donn√©es de l'entra√Ænement des mod√®les de transformation (scaler/tfidf).
    os.makedirs('models', exist_ok=True)
    os.makedirs('data/processed', exist_ok=True)
    
    # Donn√©es finales pour l'√âtape 3 (Entra√Ænement)
    joblib.dump(X_scaled, 'data/processed/X_final.joblib')
    joblib.dump(y, 'data/processed/y_final.joblib')
    
    # Sauvegarde des "Transformers" : indispensable pour traiter de nouvelles annonces 
    # de la m√™me mani√®re dans l'application finale (Inf√©rence).
    joblib.dump(vectorizer_artifacts, 'models/tfidf_svd.pkl')
    joblib.dump(scaler, 'models/scaler.pkl')

    logging.info(f"‚úÖ √âtape 2 termin√©e ! Nombre de features pr√™tes : {X_scaled.shape[1]}")

if __name__ == "__main__":
    run_step2_features()